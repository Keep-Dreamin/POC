{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "#%install_ext autotime.py\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Xavier init is well explained here in Andy's blog:\n",
    "# http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "#\n",
    "# Another good explanation can be found in Prateek Joshi's blog:\n",
    "# https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/\n",
    "def xavier_init(size):\n",
    "    input_dim = size[0]\n",
    "    xavier_variance = 1. / tf.sqrt(input_dim/2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#latent values in generation\n",
    "OUTPUT_POSSIBILITIES = 100 #100\n",
    "\n",
    "#audio window\n",
    "INPUT_DIMENSIONS = 4410 #4410 #6616 10757 #784\n",
    "\n",
    "#number of hidden nodes?\n",
    "HIDDEN_NODES = 1000 # 128\n",
    "\n",
    "#number of training iterations\n",
    "TRAINING_ITERATIONS = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10757\n"
     ]
    }
   ],
   "source": [
    "#abs\n",
    "#dataGAN = [np.abs(x.ys[:4410]) for x in testingset]#\n",
    "segment1 = np.random.rand(10757,)\n",
    "print(len(segment1))\n",
    "\n",
    "\n",
    "testingset = [\n",
    "              np.abs(segment1),\n",
    "             ]\n",
    "\n",
    "dataGAN = [np.abs(x) for x in testingset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = tf.placeholder(tf.float32, shape=[None, OUTPUT_POSSIBILITIES], name='Z')\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([OUTPUT_POSSIBILITIES, HIDDEN_NODES]), name='G_W1')\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[HIDDEN_NODES]), name='G_b1')\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([HIDDEN_NODES, INPUT_DIMENSIONS]), name='G_W2')\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[INPUT_DIMENSIONS]), name='G_b2')\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, INPUT_DIMENSIONS], name='X')\n",
    "\n",
    "#original-----\n",
    "D_W1 = tf.Variable(xavier_init([INPUT_DIMENSIONS, HIDDEN_NODES]), name='D_W1')\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[HIDDEN_NODES]), name='D_b1')\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([HIDDEN_NODES, 1]), name='D_W2')\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]), name='D_b2')\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "\n",
    "    return G_prob\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit\n",
    "\n",
    "G_sample = generator(Z)\n",
    "\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# Loss functions from the paper\n",
    "# D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "# G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# Alternative loss functions\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "# Update D(X)'s parameters\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "\n",
    "# Update G(Z)'s parameters\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "batch_size = 100 #128\n",
    "Z_dim = OUTPUT_POSSIBILITIES\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#mnist = input_data.read_data_sets('MNIST/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.shape(mnist.train.next_batch(10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.shape(mnist.train.next_batch(10)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('output/'):\n",
    "    os.makedirs('output/')\n",
    "\n",
    "i = 0\n",
    "sampleHolder = []\n",
    "\n",
    "for itr in range(TRAINING_ITERATIONS):\n",
    "    if itr % 10000 == 0:\n",
    "        \n",
    "        numSamples = 1\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_Z(numSamples, Z_dim)})\n",
    "        \n",
    "        #fig = plot(samples)\n",
    "        #plt.savefig('output/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        #i += 1\n",
    "        #plt.close(fig)\n",
    "        \n",
    "        ## construct wave from sampling during training\n",
    "        #post_reconstructions = ffttowave(   samples   ) # if using fft, transform back first?\n",
    "        sampleHolder.append(samples)\n",
    "\n",
    "\n",
    "    #CHANGHING INPUTS    \n",
    "    #X_mb, _ = mnist.train.next_batch(batch_size)\n",
    "    #X_mb = [dataGAN[0] for x in range(0,batch_size)]\n",
    "    X_mb = [ dataGAN[np.random.choice([i for i in range(0,len(dataGAN))],1)[0]] for x in range(0,batch_size)]\n",
    "    \n",
    "    #X_mb = [transforms[0]]\n",
    "\n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(batch_size, Z_dim)})\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(batch_size, Z_dim)})\n",
    "\n",
    "    if itr % 1000 == 0:\n",
    "        print('Iter: {}'.format(itr))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
